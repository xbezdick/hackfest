- name: Check update
  shell: yum check-update
  register: update
- when: update is succeeded
  block:
    # Evacuate compute node before reboot
    - block:
        - name: Get stack name
          shell: |
                  source ~/stackrc
                  openstack stack list  -f value -c "Stack Name"
          register: cloudname
        - set_fact:
              overcloud_rc: "/home/stack/{{cloudname.stdout}}rc"
    
        - name: Register storage backend type
          shell: |
              source {{ overcloud_rc }}
              openstack volume service list -f json | jq -r -c '.[] | select(.Binary | contains("cinder-volume")) | .Host' | sed s/hostgroup@tripleo_//
          register: storage_backend
    
        - name: Register hypervisor name fqdn
          shell: |
              source {{ overcloud_rc }}
              openstack hypervisor list -f value -c 'Hypervisor Hostname' | grep {{ inventory_hostname | lower }}
          register: compute_fqdn

        - name: Disable compute service on the node
          shell: |
              source {{ overcloud_rc }}
              openstack compute service set --disable --disable-reason reboot {{ compute_fqdn.stdout }} nova-compute

        - name: Check if there are nodes in build state
          shell: |
              source {{ overcloud_rc }}
              openstack server list --host {{ compute_fqdn.stdout }} -f json | jq -r -c '.[] | select(.Status | contains("BUILD")) | .Name'
          register: compute_instances_build

        - name: Sleep for one minute for each BUILD instance
          pause:
              minutes: 1
          with_items: "{{ compute_instances_build.stdout_lines }}"
    
        - name: Register instances running on compute node
          shell: |
              source {{ overcloud_rc }}
              openstack server list --host {{ compute_fqdn.stdout }} -f json | jq -r -c '.[] | select(.Status | contains("ACTIVE") or contains("PAUSED")) | .Name'
          register: compute_instances
    
        - name: Quiesce compute node
          shell: |
              source {{ overcloud_rc }}
              nova host-evacuate-live {{ compute_fqdn.stdout }}
          when: compute_instances.stdout | length > 0
    
        - name: Wait for compute node to get quiesced
          shell: |
              source {{ overcloud_rc }}
              openstack server list --host {{ compute_fqdn.stdout }} --all-projects -f json | jq -r -c '[.[] | select(.Status | contains("ACTIVE") or contains("PAUSED") or contains("MIGRATING"))] | length'
          register: compute_node_instances
          until: compute_node_instances.stdout.find("0") > -1
          retries: 30
          delay: 5
          when: compute_instances.stdout | length > 0
      delegate_to: "{{ groups['undercloud']|first }}"
      when:
        - "'compute' in group_names"
        - groups.compute|difference(groups.unused|default([]))|length > 1
    
    # Temporerily disable ceph rebalance
    - name: temporerily disable ceph rebalance
      become: true
      command: "{{ item }}"
      when: "'ceph' in group_names"
      delegate_to: "{{ groups['controller']|first }}"
      with_items:
          - 'ceph osd set noout'
          - 'ceph osd set norebalance'
    
    # check pcs cluster status
    - name: check pacemaker cluster status
      become: true
      shell: pcs status 2>&1 | grep 'cluster is not currently running on this node'
      register: pcs_active
      ignore_errors: true
    
    - name: check this if this is pcmk-remote node
      become: true
      shell: |
          pcs status 2>&1 | grep -w 'RemoteOnline:' | grep {{ inventory_hostname | lower }}
      register: pcmk_remote
      ignore_errors: true
      when: pcs_active|failed

    - name: Migrate all IPs off
      become: true
      shell: |
          hostname=$(hostname -s)
          resources=$(pcs resource show | grep IP | grep $hostname | awk '{print $1}')
          for resource in ${resources};
          do
            pcs resource move $resource --wait=300
            pcs constraint location remove cli-ban-$resource-on-$hostname
          done
      when:
          - pcs_active|failed
          - pcmk_remote|failed
    
    - name: standby pcs node
      become: true
      command: "pcs node standby --wait=300"
      when:
          - pcs_active|failed
          - pcmk_remote|failed

    - name: stop pcs cluster
      become: true
      command: "pcs cluster stop --wait=300"
      when:
          - pcs_active|failed
          - pcmk_remote|failed

    - name: load kexec kernel
      become: true
      shell: |
          KERNEL=$( rpm -q --last kernel | head -n 1 | cut -d' ' -f1 | sed  's/kernel//') ; kexec -l /boot/vmlinuz${KERNEL} --initrd /boot/initramfs${KERNEL}.img  --reuse-cmdline  --debug

    - name: reboot the node
      become: true
      shell: "sleep 5 && systemctl kexec"
      async: 1
      poll: 0
      ignore_errors: true
    
    - name: wait for node to go down
      command: ping -c1 {{ ansible_host|default(ansible_ssh_host) }}
      register: node_down
      until: node_down.rc != 0
      retries: 60
      delay: 3
      ignore_errors: true
      delegate_to: "{{ groups['undercloud']|first }}"
    
    - block:
          - name: waiting for the node to be available
            wait_for:
                port: 22
                host: "{{ ansible_host }}"
                search_regex: OpenSSH
                delay: 60
                timeout: 360
            delegate_to: "{{ groups['undercloud']|first }}"
            when: "'hypervisor' not in groups"
    
          - name: waiting for the node to be available
            become: no
            wait_for:
                host: "{{ ansible_host }}"
                port: 22
                search_regex: OpenSSH
                delay: 30
                sleep: 15
                timeout: 360
            delegate_to: hypervisor
            when: "'hypervisor' in groups"
    
    - name: check node is ssh-able
      shell: id || echo "NOT_READY"
      register: id_ssh
      until: "'{{ ansible_user }}' in id_ssh.stdout"
      retries: 20
      delay: 15
    
    - name: check pcmk_remote node is up
      become: true
      shell: pcs status | grep -w 'RemoteOnline:'
      register: remote_online
      until: "'{{ inventory_hostname | lower }}' in remote_online.stdout"
      retries: 6
      delay: 5
      when:
          - pcs_active|failed
          - pcmk_remote|succeeded
    
    # pacemaker managed services
    - name: start pcs cluster on a node
      become: true
      delay: 5
      command: pcs cluster start --wait=300
      when:
          - pcs_active|failed
          - pcmk_remote|failed

    # pacemaker managed services
    - name: unstandby pcs cluster on a node
      become: true
      delay: 5
      command: pcs node unstandby --wait=300
      when:
          - pcs_active|failed
          - pcmk_remote|failed
    
    - name: wait for pacemaker node to recover
      become: true
      shell: pcs status | grep -w 'Online:'
      register: pcs_status
      until: "'{{ inventory_hostname | lower }}' in pcs_status.stdout"
      retries: 6
      delay: 10
      when:
          - pcs_active|failed
          - pcmk_remote|failed
    
    - name: cleanup pacemaker resource
      shell: |
          pcs resource restart {{ item }} ;
          pcs resource cleanup {{ item }}
      become: true
      ignore_errors: true
      with_items:
        - "{{ cleanup_services }}"
      when:
        - cleanup_services is defined
        - cleanup_services|default({})|length > 0
        - pcs_active|failed
        - pcmk_remote|failed
    
    # Todo(yprokule): enhance search patterns
    - name: check for any stopped pcs resources
      become: true
      shell: |
          pcs status | grep {{ item }} || /bin/true
      register: srv_status
      until: srv_status.stdout.find("{{ item }}") == -1
      ignore_errors: true
      retries: 36
      delay: 5
      with_items:
          - 'Stopped:'
      when:
          - pcs_active|failed
          - pcmk_remote|failed
    
    - name: Report pacemaker status for stopped resources
      become: true
      register: fail_stop
      command: pcs status
      when:
          - srv_status|failed
    
    - name: Stopped resources found
      fail:
          msg: Some pacemeker resource failed to come back online after reboot
      when:
          - fail_stop is changed
    
    
    # Todo(yprokule): enhance search patterns
    - name: Check for any unmanaged pcs resources
      become: true
      shell: |
          pcs status | grep {{ item }} || /bin/true
      register: srv_status
      until: srv_status.stdout.find("{{ item }}") == -1
      ignore_errors: true
      retries: 36
      delay: 5
      with_items:
          - 'unmanaged'
      when:
          - pcs_active|failed
          - pcmk_remote|failed
    
    - name: Report pacemaker status for unmanaged resources
      become: true
      register: fail_unmanaged
      command: pcs status
      when:
          - srv_status|failed
    
    - name: Unmanged resources found
      fail:
          msg: Some pacemeker resource(s) unmanged after reboot
      when:
          - fail_unmanaged is changed
    
    # Re-enable ceph rebalance
    - name: reenable ceph rebalance
      become: true
      command: "{{ item }}"
      when: "'ceph' in group_names"
      delegate_to: "{{ groups['controller']|first }}"
      with_items:
          - 'ceph osd unset noout'
          - 'ceph osd unset norebalance'
    
    - name: wait for OSDs to come back
      become: true
      command: ceph pg stat
      register: active_osd
      until: active_osd.stdout.find("active+clean") > -1
      retries: 24
      delay: 15
      when: "'ceph' in group_names"
      delegate_to: "{{ groups['controller']|first }}"
    
    - name: check clock_skew on ceph cluster
      become: true
      shell: |
          ceph status | grep 'Monitor clock skew detected' || echo 'ALL_GOOD'
      register: clock_skew
      when: "'ceph' in group_names"
      delegate_to: "{{ groups['controller']|first }}"
    
    - name: check ceph cluster status
      become: true
      command: ceph status
      register: ceph_health
      until: ceph_health.stdout.find("HEALTH_OK") > -1
      retries: 30
      delay: 5
      delegate_to: "{{ groups['controller']|first }}"
      when:
          - "'ceph' in group_names"
          - "groups['ceph']|difference(groups['unused']|default([]))|length >= 3"
          - "'ALL_GOOD' in clock_skew"
    
    - name: check ceph cluster status with clock skew
      become: true
      command: ceph status
      register: ceph_health
      until: ceph_health.stdout.find("HEALTH_WARN") > -1 or ceph_health.stdout.find("HEALTH_OK") > -1
      retries: 30
      delay: 5
      delegate_to: "{{ groups['controller']|first }}"
      when:
          - "'ceph' in group_names"
          - "groups['ceph']|difference(groups['unused']|default([]))|length >= 3"

    - name: Renable the compute
      block:
        - name: migrate instance back to {{ compute_fqdn.stdout }}
          shell: |
            source {{ overcloud_rc }}
            nova live-migration {{ item }} {{ compute_fqdn.stdout }}
            openstack server show {{ item }}  -f json | jq -r -c '. | .["OS-EXT-SRV-ATTR:host"]'
          register: instance_host
          until: instance_host.stdout.find(compute_fqdn.stdout) > -1
          retries: 30
          delay: 5
          with_items: "{{ compute_instances.stdout_lines }}"

        - name: Enable compute service on the node
          shell: |
              source {{ overcloud_rc }}
              openstack compute service set --enable {{ compute_fqdn.stdout }} nova-compute

      delegate_to: undercloud-0
      when:
        - "'compute' in group_names"
        - compute_instances.stdout | length > 0
